{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07bis_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/nlpdl_project/blob/main/01_Training_PPRs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhX5n8FGOrfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcce50e-a5fb-443c-ad6e-bcb30a505e02"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sklearn\n",
        "!pip install wandb\n",
        "\n",
        "# code is based on Hauke Licht's CAP Model (https://colab.research.google.com/drive/1n7yHr0-lq-hmsXe2sqxLUhq7_4ejLH9o?usp=sharing#scrollTo=CiaoE0V8XZc6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaWLAybFblrK"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IukrS0ebonz"
      },
      "source": [
        "%env WANDB_PROJECT=press_releases_classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laGvoegZf7GO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODmZRfMZPwpu"
      },
      "source": [
        "# load and prepare data\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/nlp_project_nico/data/press_releases_lean.csv\")\n",
        "df = df.reset_index()\n",
        "df = df.dropna()\n",
        "df = df.sample(frac=0.10, random_state=123) # drop this for full model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-2BzTvCcP_5"
      },
      "source": [
        "# Creating Train, Valid and Test dataset\n",
        "\n",
        "train_size = 0.9\n",
        "train = df.sample(frac=train_size,random_state=123)\n",
        "test = df.drop(train.index).reset_index(drop=True)\n",
        "train = train.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train.shape))\n",
        "print(\"TEST Dataset: {}\".format(test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNzuQXSYlu0n"
      },
      "source": [
        "train_texts = list(train[\"text\"])\n",
        "train_labels = list(train[\"label\"])\n",
        "\n",
        "test_texts = list(test[\"text\"])\n",
        "test_labels = list(test[\"label\"])\n",
        "\n",
        "\n",
        "# there are probably better ways to do this\n",
        "ulabels = list(set(train_labels))\n",
        "train_label_dict = {}\n",
        "\n",
        "for i in range(len(ulabels)):\n",
        "  train_label_dict[str(ulabels[i])] = i\n",
        "\n",
        "train_labels = [train_label_dict[str(l)] for l in train_labels]\n",
        "\n",
        "\n",
        "# and for test set\n",
        "ulabels = list(set(test_labels))\n",
        "test_label_dict = {}\n",
        "\n",
        "for i in range(len(ulabels)):\n",
        "  test_label_dict[str(ulabels[i])] = i\n",
        "\n",
        "test_labels = [test_label_dict[str(l)] for l in test_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adOc5Q09mLjS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=1234, stratify=train_labels)\n",
        "\n",
        "print(f\"No. obs    --  train: {len(train_labels)}; validation: {len(val_labels)}; test: {len(test_labels)};\")\n",
        "print(f\"No. labels --  train: {len(set(train_labels))}; validation: {len(set(val_labels))}; test: {len(set(test_labels))};\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJASd879RNJo"
      },
      "source": [
        "# load the tokenizer \n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-german-cased\")\n",
        "\n",
        "# tokenize texts\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWvAdVf3nTwN"
      },
      "source": [
        "import torch\n",
        "class PPRDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = PPRDataset(train_encodings, train_labels)\n",
        "val_dataset = PPRDataset(val_encodings, val_labels)\n",
        "test_dataset = PPRDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdswaNK6nZvE"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, n = precision_recall_fscore_support(labels, preds, average=None)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'n': n\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STA2gdahdyx-"
      },
      "source": [
        "\n",
        "\n",
        "        Batch size: 16, 32\n",
        "        Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "        Number of epochs: 2, 3, 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIANasZInmRi"
      },
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import os\n",
        "\n",
        "#\"distilbert-base-high-lr\"\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # output directory\n",
        "    output_dir=os.path.join(\"trained\", \"distilbert_cased\", \"results\"),\n",
        "    # total number of training epochs\n",
        "    num_train_epochs=2,\n",
        "    # batch size per device during training\n",
        "    per_device_train_batch_size=16,\n",
        "    # batch size for evaluation\n",
        "    per_device_eval_batch_size=64,\n",
        "    # number of warmup steps for learning rate scheduler\n",
        "    warmup_steps=100,\n",
        "    # strength of weight decay\n",
        "    weight_decay=0.01, \n",
        "    # directory for storing logs\n",
        "    logging_dir=os.path.join(\"trained\", \"distilbert_cased\", \"logs\"),            \n",
        "    logging_steps=50,\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "    run_name=\"distilbert-base-high-lr\"\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels = len(set(test_labels)))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "wandb.log(trainer.evaluate())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBpyijY2nzqE"
      },
      "source": [
        "#\"distilbert-base-mid-lr\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # output directory\n",
        "    output_dir=os.path.join(\"trained\", \"distilbert_cased\", \"results\"),\n",
        "    # total number of training epochs\n",
        "    num_train_epochs=2,\n",
        "    # batch size per device during training\n",
        "    per_device_train_batch_size=16,\n",
        "    # batch size for evaluation\n",
        "    per_device_eval_batch_size=64,\n",
        "    # number of warmup steps for learning rate scheduler\n",
        "    warmup_steps=100,\n",
        "    # strength of weight decay\n",
        "    weight_decay=0.01, \n",
        "    # learning rate\n",
        "    learning_rate=3e-5,\n",
        "    # directory for storing logs\n",
        "    logging_dir=os.path.join(\"trained\", \"distilbert_cased\", \"logs\"),            \n",
        "    logging_steps=250,\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "    run_name=\"distilbert-base-mid-lr\"\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels = len(set(test_labels)))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    optimizers=AdamW()\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMpL50XQeab1"
      },
      "source": [
        "#\"distilbert-base-low-lr\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # output directory\n",
        "    output_dir=os.path.join(\"trained\", \"distilbert_cased\", \"results\"),\n",
        "    # total number of training epochs\n",
        "    num_train_epochs=2,\n",
        "    # batch size per device during training\n",
        "    per_device_train_batch_size=16,\n",
        "    # batch size for evaluation\n",
        "    per_device_eval_batch_size=64,\n",
        "    # number of warmup steps for learning rate scheduler\n",
        "    warmup_steps=100,\n",
        "    # strength of weight decay\n",
        "    weight_decay=0.01, \n",
        "    # learning rate\n",
        "    learning_rate=2e-5,\n",
        "    # directory for storing logs\n",
        "    logging_dir=os.path.join(\"trained\", \"distilbert_cased\", \"logs\"),            \n",
        "    logging_steps=250,\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "    run_name=\"distilbert-base-low-lr\"\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels = len(set(test_labels)))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    optimizers=AdamW()\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oifRPt8aer4G"
      },
      "source": [
        "#\"distilbert-3-epochs-high-lr\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # output directory\n",
        "    output_dir=os.path.join(\"trained\", \"distilbert_cased\", \"results\"),\n",
        "    # total number of training epochs\n",
        "    num_train_epochs=3,\n",
        "    # batch size per device during training\n",
        "    per_device_train_batch_size=16,\n",
        "    # batch size for evaluation\n",
        "    per_device_eval_batch_size=64,\n",
        "    # number of warmup steps for learning rate scheduler\n",
        "    warmup_steps=100,\n",
        "    # strength of weight decay\n",
        "    weight_decay=0.01, \n",
        "    # learning rate\n",
        "    learning_rate=5e-5,\n",
        "    # directory for storing logs\n",
        "    logging_dir=os.path.join(\"trained\", \"distilbert_cased\", \"logs\"),            \n",
        "    logging_steps=250,\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "    run_name=\"distilbert-3-epochs-high-lr\"\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels = len(set(test_labels)))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    optimizers=AdamW()\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAbDTDqbe1eA"
      },
      "source": [
        "#\"distilbert-4-epochs-high-lr\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # output directory\n",
        "    output_dir=os.path.join(\"trained\", \"distilbert_cased\", \"results\"),\n",
        "    # total number of training epochs\n",
        "    num_train_epochs=4,\n",
        "    # batch size per device during training\n",
        "    per_device_train_batch_size=16,\n",
        "    # batch size for evaluation\n",
        "    per_device_eval_batch_size=64,\n",
        "    # number of warmup steps for learning rate scheduler\n",
        "    warmup_steps=100,\n",
        "    # strength of weight decay\n",
        "    weight_decay=0.01, \n",
        "    # learning rate\n",
        "    learning_rate=5e-5,\n",
        "    # directory for storing logs\n",
        "    logging_dir=os.path.join(\"trained\", \"distilbert_cased\", \"logs\"),            \n",
        "    logging_steps=250,\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "    run_name=\"distilbert-4-epochs-high-lr\"\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels = len(set(test_labels)))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    optimizers=AdamW()\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDywCZJgfIvA"
      },
      "source": [
        "#\"distilbert-higher-batch-size\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # output directory\n",
        "    output_dir=os.path.join(\"trained\", \"distilbert_cased\", \"results\"),\n",
        "    # total number of training epochs\n",
        "    num_train_epochs=4,\n",
        "    # batch size per device during training\n",
        "    per_device_train_batch_size=32,\n",
        "    # batch size for evaluation\n",
        "    per_device_eval_batch_size=64,\n",
        "    # number of warmup steps for learning rate scheduler\n",
        "    warmup_steps=100,\n",
        "    # strength of weight decay\n",
        "    weight_decay=0.01, \n",
        "    # learning rate\n",
        "    learning_rate=5e-5,\n",
        "    # directory for storing logs\n",
        "    logging_dir=os.path.join(\"trained\", \"distilbert_cased\", \"logs\"),            \n",
        "    logging_steps=250,\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "    run_name=\"distilbert-higher-batch-size\"\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels = len(set(test_labels)))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    optimizers=AdamW()\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYConwLvqzHy"
      },
      "source": [
        "## Validation performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX8DWjgzn-8M"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "eval_res = trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSSmTbf4i6fe"
      },
      "source": [
        "evaluated = pd.DataFrame()\n",
        "evaluated[\"label\"] = [v for k,v in train_label_dict.items()]\n",
        "evaluated[\"class\"] = [k for k,v in train_label_dict.items()]\n",
        "evaluated[\"f1\"] = eval_res[\"eval_f1\"]\n",
        "evaluated[\"precision\"] = eval_res[\"eval_precision\"]\n",
        "evaluated[\"recall\"] = eval_res[\"eval_recall\"]\n",
        "evaluated[\"n\"] = eval_res[\"eval_n\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZopkEiIoD9r"
      },
      "source": [
        "evaluated.sort_values(by=\"f1\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX5e0NH9oGKx"
      },
      "source": [
        "trainer.save_model(os.path.join(\"drive\", \"MyDrive\", \"nlpdl\", \"01_PPR_model.bin\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i2ZSDllqqdW"
      },
      "source": [
        "## Test set performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-y8XZHjyAE"
      },
      "source": [
        "eval_res = trainer.predict(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4JIQ_VIJqV"
      },
      "source": [
        "eval_res.metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOFMYGIvp4Ic"
      },
      "source": [
        "evaluated = pd.DataFrame()\n",
        "evaluated[\"label\"] = [v for k,v in test_label_dict.items()]\n",
        "evaluated[\"class\"] = [k for k,v in test_label_dict.items()]\n",
        "evaluated[\"f1\"] = eval_res.metrics[\"test_f1\"]\n",
        "evaluated[\"precision\"] = eval_res.metrics[\"test_precision\"]\n",
        "evaluated[\"recall\"] = eval_res.metrics[\"test_recall\"]\n",
        "evaluated[\"n\"] = eval_res.metrics[\"test_n\"]\n",
        "\n",
        "evaluated.sort_values(by=\"f1\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozQ_m5QBEKoi"
      },
      "source": [
        "pd.crosstab(eval_res.label_ids, eval_res.predictions.argmax(-1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}