{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BaselineBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ekfqDlQLGvC8Yy-2jyfgYiounu-doF6i",
      "authorship_tag": "ABX9TyNUmYR0CTAAWmjkJSeEc1TN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/nlpdl_project/blob/main/BaselineBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhX5n8FGOrfo"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install torch\r\n",
        "!pip install sklearn\r\n",
        "\r\n",
        "# code is based on Hauke Licht's CAP Model (https://colab.research.google.com/drive/1n7yHr0-lq-hmsXe2sqxLUhq7_4ejLH9o?usp=sharing#scrollTo=CiaoE0V8XZc6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul9ImmixOk5S"
      },
      "source": [
        "# define model\r\n",
        "from transformers import BertForSequenceClassification\r\n",
        "checkpoint_model = 'distilbert-base-german-cased'\r\n",
        "model = BertForSequenceClassification.from_pretrained(checkpoint_model)\r\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODmZRfMZPwpu"
      },
      "source": [
        "# load and prepare data\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv('drive/MyDrive/germanyPPRs.csv', engine=\"python\")\r\n",
        "df = df.reset_index()\r\n",
        "df = df.dropna()\r\n",
        "# df = df.sample(1000) # drop this for full model\r\n",
        "df.date = pd.to_datetime([dt for dt in df.date], format='%Y-%m-%d')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNzuQXSYlu0n"
      },
      "source": [
        "texts = list(df[\"rawtext\"])\r\n",
        "labels = list(df[\"label\"])\r\n",
        "\r\n",
        "# there are probably better ways to do this\r\n",
        "ulabels = list(set(labels))\r\n",
        "label_dict = {}\r\n",
        "\r\n",
        "for i in range(len(ulabels)):\r\n",
        "  label_dict[str(ulabels[i])] = i\r\n",
        "\r\n",
        "labels = [label_dict[str(l)] for l in labels]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adOc5Q09mLjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd97b0c-3139-4625-94b8-2fa73fa9e881"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=.2, random_state=1234, stratify=labels)\r\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2, random_state=1234, stratify=train_labels)\r\n",
        "\r\n",
        "print(f\"No. obs    -- test: {len(test_labels)}; train: {len(train_labels)}; validation: {len(val_labels)};\")\r\n",
        "print(f\"No. labels -- test: {len(set(test_labels))}; train: {len(set(train_labels))}; validation: {len(set(val_labels))};\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. obs    -- test: 9267; train: 29652; validation: 7413;\n",
            "No. labels -- test: 6; train: 6; validation: 6;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJASd879RNJo"
      },
      "source": [
        "# load the tokenizer \r\n",
        "from transformers import DistilBertTokenizerFast\r\n",
        "\r\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-german-cased\")\r\n",
        "\r\n",
        "# tokenize texts\r\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\r\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\r\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWvAdVf3nTwN"
      },
      "source": [
        "import torch\r\n",
        "class PPRDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, encodings, labels):\r\n",
        "        self.encodings = encodings\r\n",
        "        self.labels = labels\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\r\n",
        "        return item\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.labels)\r\n",
        "\r\n",
        "train_dataset = PPRDataset(train_encodings, train_labels)\r\n",
        "val_dataset = PPRDataset(val_encodings, val_labels)\r\n",
        "test_dataset = PPRDataset(test_encodings, test_labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdswaNK6nZvE"
      },
      "source": [
        "def compute_metrics(pred):\r\n",
        "    labels = pred.label_ids\r\n",
        "    preds = pred.predictions.argmax(-1)\r\n",
        "    precision, recall, f1, n = precision_recall_fscore_support(labels, preds, average=None)\r\n",
        "    acc = accuracy_score(labels, preds)\r\n",
        "    return {\r\n",
        "        'accuracy': acc,\r\n",
        "        'f1': f1,\r\n",
        "        'precision': precision,\r\n",
        "        'recall': recall,\r\n",
        "        'n': n\r\n",
        "    }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIANasZInmRi"
      },
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\r\n",
        "import os\r\n",
        "\r\n",
        "training_args = TrainingArguments(\r\n",
        "    # output directory\r\n",
        "    output_dir=os.path.join(\"trained\", \"cap_de_distilbert_cased\", \"results\"),\r\n",
        "    # total number of training epochs\r\n",
        "    num_train_epochs=3,\r\n",
        "    # batch size per device during training\r\n",
        "    per_device_train_batch_size=16,\r\n",
        "    # batch size for evaluation\r\n",
        "    per_device_eval_batch_size=64,\r\n",
        "    # number of warmup steps for learning rate scheduler\r\n",
        "    warmup_steps=500,\r\n",
        "    # strength of weight decay\r\n",
        "    weight_decay=0.01,     \r\n",
        "    # directory for storing logs\r\n",
        "    logging_dir=os.path.join(\"trained\", \"distilbert_cased\", \"logs\"),            \r\n",
        "    logging_steps=250,\r\n",
        ")\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB3yVob4noDR",
        "outputId": "447bf0ff-22ba-4daf-a2ac-c203435545fd"
      },
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-german-cased\", num_labels = 21)\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    model=model,\r\n",
        "    args=training_args,\r\n",
        "    compute_metrics=compute_metrics,\r\n",
        "    train_dataset=train_dataset,\r\n",
        "    eval_dataset=val_dataset\r\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-german-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBpyijY2nzqE"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX8DWjgzn-8M"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\n",
        "eval_res = trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZopkEiIoD9r"
      },
      "source": [
        "evaluated.sort_values(by=\"f1\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX5e0NH9oGKx"
      },
      "source": [
        "trainer.save_model(os.path.join(\"trained\", \"distilbert_cased\", \"results\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}