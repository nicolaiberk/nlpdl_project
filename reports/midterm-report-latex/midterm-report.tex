\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{statcourse}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}


\statcoursefinalcopy


\setcounter{page}{1}
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DO NOT EDIT ANYTHING ABOVE THIS LINE
% EXCEPT IF YOU LIKE TO USE ADDITIONAL PACKAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%% TITLE
\title{Midterm Project Report for Newspaper Bias project}

\author{Tom Arend\\
{\tt\small t.arend@phd.hertie-school.org}
\and
Nicolai Berk\\
{\tt\small nicolai.berk@gmail.com}
}

\maketitle
%\thispagestyle{empty}


% MAIN ARTICLE GOES BELOW
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%% ABSTRACT
\begin{abstract}
   The measurement of ideology is one of the major applications of text analysis in political science. However, researchers often face scarcity of available labelled data to train supervised models for their specific domain. Manual annotation is costly and often severely affected by subjective bias. We propose to fine-tune transformer models on available, labelled political texts issued by political parties to obtain a classifier of political ideology. Using a unique dataset of newspaper articles authored by politicians, we test such an application in the German context. Comparing transformer neural networks fine-tuned on a set of party press releases, a set of newspaper articles, or both, we present evidence on the feasibility of such an approach. This contributes to the broad literature on text analysis in the political domain, enabling researchers to train powerful deep learning models on political language with scarce training data. Additionally, we contribute a state-of-the-art deep learning model for the measurement of ideological bias in news articles. This is a crucial issue in the debates surrounding media effects on polarisation, turnout, and voting behaviour.\footnote{GitHub repository: \url{https://github.com/nicolaiberk/nlpdl\_project}}
\end{abstract}

 
%%%%%%%%% BODY TEXT

% \begin{itemize}
% {\color{blue}

% \item Remember that you should \textbf{submit the report}  via Moodle and \textbf{include in the report the link to accessible GitHub repository that contains the code}. Also, \textbf{only one member per team} needs to submit the project material. You must include a link to your GitHub repository for the project as the first footnote on the first page. 

% \item The midterm project report should be {\bf 4 pages long (not counting references), and a maximum 10 references}. The report should contain the sections that are already provided in this paper. It forms the basis of the final report with the same structure. Please check out the text in these sections for further information.

% \item Your midterm milestone will be graded on the following criteria:
% \begin{itemize}
% \item Progress: Has the team made good progress on the project? You should have done approximately half of the work of your project.
% \item As a minimum, your milestone should show that you have setup your data, baseline model code, and evaluation metric, and run experiments to obtain some results (assuming you are doing a typical model-building project). Other than this, `good progress' depends on various factors (e.g., whether your model is implemented from scratch or based on an existing codebase).
% \item Understanding: Does the milestone show a strong understanding of its problem, tasks, methods, metrics, and research context?
% \item Writing quality: Does the milestone clearly communicate what you've done and why, providing the requested information, to an appropriate level of detail (given the page limit)?
% \item You will receive some brief feedback on your milestone. Feedback may contain helpful suggestions for your project (e.g., try a particular method, read a particular paper) and/or warnings about your project plan (e.g., if your plans are too ambitious or not ambitious enough), and how you could improve your technical writing (e.g., adjustments to clarity, level of detail, formatting, use of references).
% \end{itemize}

% \item Technical writing is an important skill in this class, in research, and beyond. It's well worth spending time developing your ability to communicate technical concepts clearly. Here are some resources which might help you improve your technical writing:
% \begin{itemize}
% \item Tips for Writing Technical Papers, Jennifer Widom (\url{https://cs.stanford.edu/people/widom/paper-writing.html}).
% \item Write the Paper First, Jason Eisner (\url{https://www.cs.jhu.edu/~jason/advice/write-the-paper-first.html}).
% \end{itemize}

% \item Here are some other things you can do to improve your technical writing:
% \begin{itemize}
% \item Look carefully at several ML / NLP papers to understand their typical structure, writing style, and the usual content of the different sections. Model your writing on these examples.
% \item Think about the NLP / ML papers you've read (for example, the one you summarised for your proposal). Which parts did you find easy to understand and why? Which parts did you find difficult to understand and why? Can you identify any good writing practices that you could use in your technical writing?
% \item Ask a friend to read through your writing and tell you if is clear. This can be useful even if the friend does not have the relevant technical knowledge.
% \end{itemize}
% }
% \end{itemize}

\section{Introduction}



% This section details your approach(es) to the problem. For example, this is where you describe the architecture of your model, and any other key methods or algorithms.

% You should also describe your baseline(s). Depending on space constraints, and how standard your baseline is, you might do this in detail, or simply refer the reader to some other paper for the details. 

% If any part of your approach is original, make it clear (so we can give you credit!). For models and techniques that aren't yours, provide references.

% If you're using any code that you didn't write yourself, make it clear and provide a reference or link. When describing something you coded yourself, make it clear (so we can give you credit!).

The measurement of ideology and political bias\footnote{'Political bias', 'ideological slant' and variations of the two are used interchangeably in this report.} are the subject of much research on political texts \cite{Bilbao-Jayo2018, Laver2003, Simoes2020}. Despite significant advances in our understanding and detection of ideology, most researchers still face significant constraints when working with text. Notably, they face a lack of appropriately labelled training data. This is linked to the high costs incurred by manually annotating a significant number of speeches, texts, or sentences. Often, the detection of ideological bias might be highly dependent on the coders' subjective assessment. Instead, researchers could train models on other sources of text with clear and available labels and subsequently apply them to the desired texts using transfer learning.

We conduct experiments with different hyperparameters on several transformers model to understand if and how transfer learning can be used to measure bias in the absence of abundant training data. To test this, we fine-tune DistilBert (\cite{}) on German press releases. Once the model is fine-tuned to predict the authoring party of press releases, it is applied to estimate the bias of newspaper. This bias is conceptualized as the probability of a given article being authored by one of the German parties. While the application of transformers to measure bias is not entirely new in political science \cite{Terechshenko2020, Vig2020}, we move beyond the current state-of-the-art by measuring the precise implications of different fine-tuning procedures. 

Beyond testing the effectiveness of this two-step fine-tuning process, this project will develop a state-of-the-art deep learning model for the measurement of political bias in newspaper articles.  Newspapers represent an important institution in the political world, affecting phenomena ranging from polarization to voter turnout. Much like the shadows in Plato's cave allegory, news provide elites and citizens with a representation of a reality they are not able to see themselves \cite{Plato520a}. The media have the power to affect voting behaviour \cite{Chiang2011, Ladd2009}, as well as polarise the electorate \cite{Lelkes2017} or motivate them to turn out to vote \cite{Gentzkow2010}. 

Given the importance of the news media for the study of politics, it is surprising that few papers deploy state-of-the-art deep learning technologies to classify ideological bias in news articles. Gentzkow and Shapiro estimate slant in US newspapers by identifying bi- and trigrams' indicative of a congressional speakers' party, and apply the resulting dictionary to newspapers to scale them \cite{Gentzkow2010}. More recently, Widmer et al. have assessed polarisation in the US media environment using a supervised model. They train a classifier on bigrams, predicting whether content was produced by a left-leaning network (CNN) or a right-leaning network (Fox news) \cite{Widmer2020}. We believe both approaches are likely inferior to more complex deep learning models, as such novel approach would not incorporate idiosyncratic phrases used by the specific networks identified to train the data. Using party labels to train classifiers is more straightforward, as it places newspapers within the existing context. If existing approaches to classify newspaper slant can be improved upon, or even just complemented, we could provide an additional tool for researchers to study drivers and effects of media bias. A working state-of-the-art model might even renew interest in the subject matter and encourage researchers to find new and exciting applications for it. 

Outside of academia, a confident and robust classifier of newspaper bias might help readers to identify when they are reading an article that is overly partisan. This would perhaps encourage them to approach certain news sources with more scepticism and hold news outlets to higher editorial standards. In the long run, the highlighting of biases in articles might counteract the worrying polarization of entire electorates. \\

\section{Proposed Method}

We propose the use of transfer learning to identify newspaper slant: a pre-trained transformers model is fine-tuned to identify the authoring party of press releases. Then, this model is applied to several newspaper articles, indicating which parties' communication an article most resembles. This approach is useful, as political parties represent the major organisers of and reference points for political ideology. More importantly, party-issued political texts constitute data that already carry ideological labels and need no further labour for annotation. It therefor represents an efficient and broadly available source of data for transfer learning to identify ideological bias in newspaper articles. 'Bias' is here defined as the similarity of a set of text to a given party's communication. This is a reasonable definition, as political parties constitute the major points of reference and organizers of ideology in contemporary democracies. 

In detail, we use the German language pre-trained DistilBert model \footnote{Link to the model page: \url{https://huggingface.co/transformers/model_doc/distilbert.html}} and compare how different hyperparameters and/or the addition of more complex layers affect the model's classification performance. We expect optimal performance from a transformer neural network that was fine-tuned in two steps. First we use fine-tune the DistilBert model on German party press releases. This in essence resembles a by issuing party. Secondly, the model is applied in its actual domain to estimate ideological bias in news articles. We fine-tune and subsequently validate the model using op-eds by politicians. These unique data have the advantage of carrying party labels, allowing a direct transfer of the categories from the initial fine-tuning process (first step) to the outcome of interest. We compare this fine-tuned model to a BERT model that was not fine-tuned on party press releases and one that was not fine-tuned on newspaper data. This leaves us with three models:

\begin{enumerate}
    \item A model only fine-tuned on newspaper articles authored by politicians.
    \item A model only fine-tuned on party press releases.
    \item A model both fine-tuned on press releases and politicians' newspaper contributions.
\end{enumerate}

This approach  assess by how much model performance is improved upon when including information from both party press releases and newspaper data. This is especially useful as it allows us to comparatively assess the performance of a model that has never seen data from the domain of interest (model 2). If this model performs well, this constitutes strong evidence that party communication can be used to train classifiers which are applied in a diverse set of domains with scarce or no training data. If model 3 severely outperformed model 1, this would corroborate the idea that information from available sets of texts from political actors can be used to improve ideology measurements for other sets of text.  \\

\section{Experiments}

\paragraph{Data:} In this project, we draw on three distinct data sources:\\
\begin{itemize}
    \item A dataset of over 40,000 German party press releases issued between 2010 and 2019, collected by the SCRIPTS project\footnote{\url{https://www.scripts-berlin.eu/index.html}. Special thanks go to Lukas Stötzer for the effortless (for us) provision of the data.}.
    \item A collection of over 2 million German newspaper articles from six major newspapers, published between 2013 and 2019, collected by one of the authors in a previous project (\cite{Krause2021}).
    \item A set of German newspaper articles authored by politicians collected as part of this project (collection underway).
\end{itemize}

\paragraph{Evaluation method:} So far, we have successfully fine-tuned a BERT-model to classify party-press releases by authorship (model 2, serving as the basis for model 3). The performance of this model is shown in \ref{fig:party}. As the reader can see, the model performs at a very high, near-perfect level. We believe that the classification performance made hyper-parameter optimisation at this stage somewhat obsolete. It might however be useful in the second stage of fine-tuning. The impressive performance of this model on the press releases was rather surprising to the authors and shifted the initial focus of the project away from improving the classification of party press releases to the comparison of improvements in the two-step fine-tuning process. \\

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{reports/midterm-report-latex/figures/party_class.png}
    \caption{Performance of model 2 for the classification of party press releases.}
    \label{fig:party}
\end{figure}

For the final assessment of classifier performance, we will use a unique, newly collected set of politicians' op-eds and interviews. This data enables us to compare the impact of different fine-tuning processes (using party press releases, newspaper articles, or both) on model performance, as the outcome categories (authorship party) are identical. We aim for a balance of precision and recall, and will therefore evaluate the models' F1-scores. Additionally, we will assess generalisability by assessing the placement of newspapers based on estimates for all news articles. Left-wing newspapers such as the TAZ should be placed closer to left-wing parties (Grüne, SPD, Linke) than right-wing newspapers (FAZ, WELT).

\paragraph{Experimental details:} 

% How you ran your experiments (e.g. model configurations, learning rate, training time, etc.)

We use the 'distilbert-base-german-cased' model from the Huggingface transformer library\footnote{\url{https://huggingface.co/bert-base-german-cased}.} for all three models. This deep-learning model has been trained to efficiently solve different classification tasks on 12 GB of German language data, including Wikipedia, legal data, and news. Given the impressive performance on the party press releases, deviation from the default settings for most hyper-parameters was deemed unnecessary thus far. The same applies for the use of the larger, more powerful, but slower 'bert-base-german-cased' model. We decided on three training epochs, a training batch size of 16, and a weight decay of $0.01$ for regularisation. This might change in the future, depending on the model's performance on the newspaper articles.\\

The configuration of our hyper-parameters hence largely follows the defaults and is as follows\footnote{The model's default settings can be found here: \url{https://huggingface.co/distilbert-base-german-cased/blob/main/config.json}.}:

\begin{itemize}
    \item Training epochs: $3$
    \item Training batch size: $16$
    \item We apply a weight decay as a form of regularisation: $0.01$
    \item Maximum sequence length (longer articles were truncated): $512$
    \item Dropout for each layer: $10\%$
\end{itemize}

\paragraph{Results:} We classified a training set of 4,000 newspaper articles using model 2 (fine-tuned on party press releases). We expected articles from conservative newspapers FAZ and Welt to be more similar to Union and FDP and possibly the AfD, especially compared to the progressive TAZ. Spiegel Online (SPON) is expected to be equally similar to right- and leftwing newspapers.\\

The results can be seen in table \ref{tbl:means}, which shows the average probability for an article to be classified as being authored by a given party. As expected, FAZ and Welt are very similar to Union (81\%) and FDP (58\%/55\%), but also rather similar to the Greens (36\%/35\%). While they show the highest similarity to the AfD (8\%/7\%), similarity to the radical-right party is generally on a very low level among all newspapers. Spiegel Online (SPON) shows lower similarity to the FDP and closer to Union and Greens, but is generally rather similar to the right-wing newspapers. As expected, the TAZ shows a comparatively different profile, being very similar to the Greens (average likelihood 70\%), and less similar to the Union parties (41\%), the FDP (32\%), and the AfD (5\%). Surprisingly, it also shows the lowest similarity to the Linke (13\%). Maybe most surprising is the general low similarity to SPD press releases (4\%/5\%). It seems the party has a rather distinctive style in its press releases.\\

\begin{table}[!htbp] \centering 
    \begin{tabular}{@{\extracolsep{1pt}} lcccccc}
    \\[-1.8ex]\hline 
    \hline \\[-1.8ex] 
    Paper & Linke & Grüne & SPD & Union & FDP & AfD \\ 
    \hline \\[-1.8ex] 
    FAZ & $0.15$ & $0.36$ & $0.05$ & $0.81$ & $0.58$ & $0.08$ \\ 
    Welt & $0.15$ & $0.35$ & $0.05$ & $0.81$ & $0.55$ & $0.07$ \\ 
    Spiegel & $0.11$ & $0.39$ & $0.05$ & $0.89$ & $0.34$ & $0.07$ \\ 
    TAZ & $0.13$ & $0.69$ & $0.04$ & $0.41$ & $0.32$ & $0.05$ \\ 
    \hline \\[-1.8ex] 
    \end{tabular} 
  \caption{Mean similarity estimate to each party by newspaper.} 
  \label{tbl:means} 
\end{table}

Given that model 2 is likely the worst performing model, we consider these results to be promising at this early stage. However, we need to look into the odd performance of the SPD category, which we expected to be generally well-represented in media articles, especially in the left-wing TAZ.

\section{Future work}

Given the surprisingly good performance of the baseline model on the classification of party press releases, the project can move on to focus on the evaluation of the effectiveness of the two-step fine-tuning process as its major contribution. We identified several key tasks to meet this goal\footnote{The open issues can be found via \url{https://github.com/nicolaiberk/nlpdl\_project/issues}.}.

The collection of several hundred newspaper articles and interviews by politicians for fine-tuning and model evaluation is the most important and urgent next step. This is done mostly by scraping politicians' websites. Once this data is available, we will be able to fine-tune model 1 and model 3 and comparatively assess the performance of all three models on a held-out set. This will enable us to assess which model performs best, and - more importantly - how much the model is improved upon by each fine-tuning step.

Even though the assessment of the models' performance does constitute the major goal of the project, several additional tasks are conceivable in order for us to fully grasp the necessary model improvements. The extremely accurate performance of the model 2 in the classification of press releases might fit certain characteristics we do not want it to learn. We intend to assess this by using a test set that lies in the future, meaning beyond the end-date of the training set currently in use. This way, we can see whether the model's high accuracy owes to idiosyncrasies of party press releases at a given period in time (same topics, same politicians addressed). Should accuracy drop substantially, the model might suffer from overfitting and the classification accuracy might be due to the incorporation of time-specific noise. This is likely only necessary if model 2 performs rather badly on the test set of politicians op-eds.

In the same vein, the surprisingly low classification rate for news articles resembling the SPD will have to be investigated. This might entail an in-depth look at the input data. We will most likely take a qualitative approach to understand whether the SPD's press releases are radically different from others or whether the party is really under-represented in the coverage of these newspapers.

There are some additional tasks that we would like to address should time permit it. Chief among these is extensive hyper-parameter optimisation to improve the performance of all models. More importantly, we might make additional efforts to understand \textit{why} additional fine-tuning steps could be important (or not). One potential avenue to understand how different fine-tuning processes affect our models, we might build on recent work employing causal mediation analysis to assess the interaction of the structure and predictions in transformer models \cite{Vig2020}.



\section{Conclusion}

As it stands, we are confident that the project is on the right track, despite the necessary shift in focus. While the surprising performance of the initial baseline model might have derailed our original efforts, we believe that we managed to successfully shift the project in a direction that promises to result in an original and ambitious contribution to the literature. We strongly believe that there is a value in pursuing an approach that is more focused on the transfer learning aspect than on the initial fine-tuning. Not only does it address one of the crucial shortcomings of deep learning (i.e. a need for large amounts of labelled data), but also opens up the benefits to wider applications. We are eager to hear your feedback regarding both the results thus far and our ideas for the experiments addressing the issue of fine-tuning transformers in the absence of appropriate training data.

{\small
\bibliographystyle{ieee}
\bibliography{bibliography.bib}
}

\end{document}
